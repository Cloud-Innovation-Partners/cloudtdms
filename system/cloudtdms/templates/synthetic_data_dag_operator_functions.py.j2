{% block operator_functions %}

def generate_iterator(data_frame, methods,args_array):
    number = dag.params.get('stream').get('number')
    for fcn, name in methods:
        func_name = fcn.__name__
        arg = args_array.get(func_name)
        if arg is None:
            result = fcn(data_frame, number)
        else:
            result = fcn(data_frame,number, arg)
        # data_frame[name] = pd.Series(result)


def data_generator(**kwargs):
    stream = dag.params.get('stream')
    meta_data = providers.get_active_meta_data()
    format = stream.get('format', None)
    format = 'csv' if format is not None and str(format).lower() == 'csv' else 'json' if format is not None and str(format).lower() == 'json' else 'csv'
    attributes = dag.params.get('attributes') if dag.params.get('attributes') is not None else {}

    # check 'schema' attribute is present
    schema = stream['schema'] if 'schema' in stream else []
    list(map(lambda x: x.update({"field_name": f"schema.{x.get('field_name')}"}), schema))

    for source_type in dag.params.get('source').keys():
        for connection in dag.params.get('source').get(source_type):
            cname = connection.get('connection')
            # check 'source' attribute is present
            file_name = f"{source_type}_{cname}_{dag.owner}_{stream['title']}_{str(kwargs['execution_date'])[:19].replace('-','_').replace(':','_')}.csv"
            source = f'{get_user_data_home()}/.__temp__/{file_name}' if 'source' in stream else None

            # columns in data-file
            all_columns = []
            if source is not None:
                try:
                    all_columns = pd.read_csv(source, nrows=1).columns
                except FileNotFoundError:
                    LoggingMixin().log.error(f'ValueError: File {source} not found')

            # get columns to delete
            delete = stream['delete'] if 'delete' in stream else []

            all_columns=[f for f in all_columns if f not in delete]

            # check 'substitute' attribute is present along with 'source'
            if 'substitute' in stream and source is not None:
                substitutions = []
                for k, v in stream['substitute'].items():
                    v['field_name'] = k
                    substitutions.append(v)

                schema += substitutions

            # check 'encrypt' attribute is present along with 'source'

            if 'encrypt' in stream and source is not None:
                encryption = [{"field_name": v, "type": "advanced.custom_file", "name": file_name, "column": v,
                               "ignore_headers": "no", "encrypt": {"type": stream['encrypt'].get(v).get('type'), "key": stream['encrypt'].get(v).get("encryption_key")}}
                              for v in stream['encrypt'].keys() if v in all_columns]
                schema += encryption

            # check 'mask_out' attribute is present along with 'source'

            if 'mask_out' in stream and source is not None:
                mask_outs = [{"field_name": k, "type": "advanced.custom_file", "name": file_name,
                              "column": k, "ignore_headers": "no", "mask_out": {"with": v['with'], "characters": v['characters'], "from": v["from"]}}
                             for k, v in stream['mask_out'].items() if k in all_columns]
                schema += mask_outs

            # check 'shuffle' attribute is present along with 'source'

            if 'shuffle' in stream and source is not None:

                shuffle = [{"field_name": v, "type": "advanced.custom_file", "name": file_name, "column": v,
                            "ignore_headers": "no", "shuffle": True} for v in stream['shuffle'] if v in all_columns]
                schema += shuffle

            # check 'nullying' attribute is present along with 'source'

            if 'nullying' in stream and source is not None:

                nullify = [{"field_name": v, "type": "advanced.custom_file", "name": file_name, "column": v,
                            "ignore_headers": "no", "set_null": True} for v in stream['nullying'] if v in all_columns]
                schema += nullify

            if source is not None:
                schema_fields = [f['field_name'] for f in schema]
                remaining_fields = list(set(all_columns) - set(schema_fields))
                remaining = [{"field_name": v, "type": "advanced.custom_file", "name": file_name, "column": v,
                                "ignore_headers": "no"} for v in remaining_fields if v in all_columns]
                schema += remaining

            stream['schema'] = schema

        if not schema:
            LoggingMixin().log.error(f"AttributeError: attribute `schema` not found or is empty in {name}.py")
            # continue

        # Remove Duplicates In Schema
        new_schema = {}
        for s in schema:
            new_schema[s['field_name']] = s
        schema = list(new_schema.values())

        for col in [f['field_name'] for f in schema]:
            if col not in all_columns:
                all_columns.append(col)

        stream['original_order_of_columns'] = all_columns

        schema.sort(reverse=True, key=lambda x: x['type'].split('.')[1])

        for scheme in schema:
            data, column = scheme['type'].split('.')
            if data in meta_data['data_files']:
                if column in meta_data['meta-headers'][data]:
                    if data not in attributes:
                        attributes[data] = [column]
                    else:
                        attributes[data].append(column)
                else:
                    raise AirflowException(f"TypeError: no data available for type {column} ")
            elif data in meta_data['code_files']:
                if column in meta_data['meta-functions'][data]:
                    if data not in attributes:
                        attributes[data] = [column]
                    else:
                        attributes[data].append(column)
                else:
                    raise AirflowException(f"TypeError: no data available for type {column} ")
            else:
                raise AirflowException(f"IOError: no data file found {data}.csv ")

    locale=dag.params.get('stream').get('locale')

    nrows = int(stream['number'])
    ncols = sum([len(f) for f in attributes.values()])
    columns = []
    labels = [columns.extend(f) for f in attributes.values()]
    data_frame = pd.DataFrame(pd.np.zeros((nrows, ncols))*pd.np.nan, columns=[v + str(columns[:i].count(v)) if columns.count(v) > 1 and columns[:i].count(v) != 0 else v for i, v in enumerate(columns)])

    for attrib in attributes:
        if attrib in meta_data['data_files']:
            try:
                df = pd.read_csv(list(pathlib.Path(get_providers_home()).rglob(f"{attrib}.csv")).pop(0), usecols=[column for (field_name, column) in attributes[attrib]])
            except (FileNotFoundError, IndexError):
                df = pd.read_csv(f"{os.path.dirname(get_airflow_home())}/user-data/{attrib}.csv", usecols=[column for (field_name, column) in attributes[attrib]])

            df_temp = pd.DataFrame(index=range(nrows), columns=[column for (field_name, column) in attributes[attrib]])
            for i in range(nrows):
                df_temp.iloc[i] = df.iloc[random.randrange(len(df))]

            data_frame[[column for (field_name, column) in attributes[attrib]]] = df_temp[[column for (field_name, column) in attributes[attrib]]]
        elif attrib in meta_data['code_files']:
            mod = importlib.import_module(f"system.cloudtdms.providers.{attrib}")
            args_array={f"{f['field_name']}-$-{f['type'].split('.')[1]}": {k: v for k, v in f.items() if k not in ('field_name', 'type')} for f in schema if f.get('type').startswith(attrib)}
            try:
                args_array['locale']=locale
                _all = getattr(mod, attrib)
                _all(data_frame, nrows, args_array)
            except AttributeError:
                # args_array={f['type'].split('.')[1]: {k: v for k, v in f.items() if k not in ('field_name', 'type')} for f in schema if len(f) > 2}
                methods = [(getattr(mod, m), m) for m in attributes[attrib]]
                generate_iterator(data_frame, methods,args_array)
    file_name = f"{stream['title']}_{str(kwargs['execution_date'])[:19].replace('-','_').replace(':','_')}"
    try:
        data_frame = data_frame[stream['original_order_of_columns']]

        # Attribute output_schema

        output_schema = stream['output_schema'] if 'output_schema' in stream else {}

        # truncate dataframe
        try:
            data_frame.drop(columns=[f'{f}' for f in data_frame.columns if f not in output_schema.keys()], inplace=True, axis=1)
        except KeyError:
            LoggingMixin().log.warn(exc_info=True)

        # rename columns
        data_frame.columns = list(map(lambda x: f"{os.path.splitext(x)[0]}.{output_schema.get(x)}" if x in output_schema else  x, data_frame.columns))

        data_frame.columns = list(map(lambda x: x if os.path.splitext(x)[1] == '' else os.path.splitext(x)[1][1:], data_frame.columns))

      # reorder
        data_frame = data_frame.reindex(output_schema.values(), axis=1)

        # reduced data_frame
        vector_length = np.vectorize(len)
        for column in data_frame.columns:
            if int(vector_length(data_frame[column].astype(str)).max(axis=0)) > 255:
                data_frame[column] = data_frame[column].astype(str).str[:255]

        if stream['format'] == 'csv':
            data_frame.to_csv(f"{get_output_data_home()}/{dag.owner}/{stream['title']}/{file_name}.csv", index=False)
        elif stream['format'] == 'json':
            data_frame = data_frame.loc[:,~data_frame.columns.duplicated()]
            data_frame.to_json(f"{get_output_data_home()}/{dag.owner}/{stream['title']}/{file_name}.json", orient='records', lines=True)

    except FileNotFoundError:
        os.makedirs(f"{get_output_data_home()}/{dag.owner}/{stream['title']}")

        if stream['format'] == 'csv':
            data_frame.to_csv(f"{get_output_data_home()}/{dag.owner}/{stream['title']}/{file_name}", index=False)
        elif stream['format'] == 'json':
            data_frame.to_json(f"{get_output_data_home()}/{dag.owner}/{stream['title']}/{file_name}", orient='records', lines=True)

    finally:
        for source_type in dag.params.get('source').keys():
            for connection in dag.params.get('source').get(source_type):
                cname = connection.get('connection')
                file_name = f"{source_type}_{cname}_{dag.owner}_{stream['title']}_{str(kwargs['execution_date'])[:19].replace('-','_').replace(':','_')}.csv"
                path = f'{get_user_data_home()}/.__temp__/{file_name}'
                try:
                    os.remove(path)
                    LoggingMixin().log.info(f"Deleted : {file_name}")
                except FileNotFoundError:
                    LoggingMixin().log.warn("No file found to delete!")

{% endblock operator_functions %}